{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Syed Razvi Project 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "import pickle\n",
    "import datetime as dt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtaining Data from myanimelist.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of season URL's to pull show links from starting with year x going to end of 2020\n",
    "x = 2010\n",
    "season_list=[]\n",
    "seasons = ['winter','spring','summer','fall']\n",
    "for i in range(x,2021):\n",
    "    for j in seasons:\n",
    "        season_list.append(str(i)+'/'+j)\n",
    "print(season_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull individual show links from season pages\n",
    "url = \"https://myanimelist.net/anime/season/{}\"\n",
    "show_links = []\n",
    "for i in season_list:\n",
    "    url2 = url.format(i)\n",
    "    season_page = requests.get(url2)\n",
    "    season_soup = BeautifulSoup(season_page.text)\n",
    "    for link in season_soup.find_all('div',class_='seasonal-anime js-seasonal-anime'):\n",
    "        if link.find('div',class_='info').text.strip('\\n ')[:2] == 'TV':\n",
    "            show_links.append(link.find('a',class_='link-title').get('href'))\n",
    "    time.sleep(.5+2*random.random())\n",
    "show_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing Duplicate Show Links\n",
    "show_links2 = list(dict.fromkeys(show_links))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Data from Individual Show Pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping Individual Show Data\n",
    "Df_Row = []\n",
    "for show in show_links2:\n",
    "    show_page = requests.get(show+'/stats')\n",
    "    show_soup = BeautifulSoup(show_page.text)\n",
    "    Genres = []\n",
    "    title = show_soup.title.text.strip('\\n').replace(' - Statistics - MyAnimeList.net','')\n",
    "    for box in show_soup.find('div', style='width: 225px').find_all('div'):\n",
    "        if 'Type:' in box.text:\n",
    "            media = box.text.replace('Type:','').strip('\\n ')\n",
    "        if 'Episodes:' in box.text:\n",
    "            episodes = box.text.replace('Episodes:','').strip('\\n ')\n",
    "        if 'Status:' in box.text:\n",
    "            status = box.text.replace('Status:','').strip('\\n ')\n",
    "        if 'Aired:' in box.text:\n",
    "            aired = box.text.replace('Aired:','').strip('\\n ')\n",
    "        if 'Broadcast:' in box.text:\n",
    "            broad = box.text.replace('Broadcast:','').strip('\\n ')\n",
    "        if 'Licensors:' in box.text:\n",
    "            license = box.text.replace('Licensors:','').strip('\\n ')\n",
    "        if 'Source:' in box.text:\n",
    "            source = box.text.replace('Source:','').strip('\\n ')\n",
    "        if 'Genres:' in box.text:\n",
    "            for genre in box.find_all('span', itemprop='genre'):\n",
    "                Genres.append(genre.text)\n",
    "        if 'Duration:' in box.text:\n",
    "            duration = box.text.replace('Duration:','').strip('\\n ')\n",
    "        if 'Rating:' in box.text:\n",
    "            rating = box.text.replace('Rating:','').strip('\\n ')\n",
    "        if 'Score:' in box.text:\n",
    "            try:\n",
    "                score = float(box.find('span', itemprop='ratingValue').text)\n",
    "            except:\n",
    "                score = float(0)\n",
    "    for i in show_soup.find_all('div', class_='spaceit_pad'):\n",
    "        if 'Watching:' in i.text:\n",
    "            watched = int(i.text.replace('Watching: ','').replace(',',''))\n",
    "        if 'Completed:' in i.text:\n",
    "            completed = int(i.text.replace('Completed: ','').replace(',',''))\n",
    "        if 'Dropped:' in i.text:\n",
    "            dropped = int(i.text.replace('Dropped: ','').replace(',',''))\n",
    "        if 'Total:' in i.text:\n",
    "            total = int(i.text.replace('Total: ','').replace(',',''))\n",
    "    Row_List = [title,media, episodes,status,aired,broad,license,source,Genres,duration,rating,watched,completed,dropped,total,score]\n",
    "    Df_Row.append(Row_List)\n",
    "    time.sleep(.5+2*random.random())\n",
    "Df_Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Scraped data list into dataframe\n",
    "Full_DF = DataFrame(Df_Row,columns=['Title','Media_Type','Episodes','Status','Aired','Broadcast','Licensed','Source','Genres','Runtime','Age_Rating','Watched','Completed','Dropped','Total','Score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning up and Formatting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Focus on shows that have finished airing by removing currently airing or not yet aired shows\n",
    "Data_Set = Full_DF[Full_DF['Status'] == 'Finished Airing']\n",
    "Data_Set['Episodes'] = Data_Set['Episodes'].astype(int)\n",
    "Data_Set = Data_Set.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dummy Variables for Age Rating\n",
    "Data_Set = pd.concat([Data_Set.drop('Age_Rating', axis=1), pd.get_dummies(Data_Set['Age_Rating'])], axis=1)\n",
    "Data_Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixing Runtimes to be Numerical\n",
    "x='40'\n",
    "Data_Set['Runtime'] = Data_Set['Runtime'].replace(x+'/60','0')\n",
    "Data_Set['Runtime'] = Data_Set['Runtime'].astype(int)\n",
    "Data_Set['Runtime'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adaptation Source Dummy\n",
    "Data_Set['Source'] = Data_Set['Source'].replace('Radio','Other')\n",
    "Data_Set['Source'].value_counts()\n",
    "Data_Set = pd.concat([Data_Set.drop('Source', axis=1), pd.get_dummies(Data_Set['Source'])], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Licensed or Not Dummy\n",
    "Data_Set['Licensed'].value_counts()\n",
    "licensed = []\n",
    "for row in Data_Set['Licensed']:\n",
    "    if row =='None found, add some':\n",
    "        licensed.append(0)\n",
    "    else:\n",
    "        licensed.append(1)\n",
    "licensed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding Watch Stats\n",
    "Data_Set['Perc_Watched'] = (Data_Set['Watched'] + Data_Set['Completed'])/Data_Set['Total']\n",
    "Data_Set['Perc_Dropped'] = (Data_Set['Dropped'])/Data_Set['Total']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing Unused Variables\n",
    "Data_Set2 = Data_Set.drop(['Title','Media_Type','Status','Aired','Broadcast','Licensed','Genres','Watched','Completed','Dropped','Action'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing Unscored Shows or shows with too few votes to have been given a score\n",
    "Data_Set2 = Data_Set2[Data_Set2['Score'] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pickling\n",
    "with open('cleanscrape2.pickle', 'wb') as to_write:\n",
    "    pickle.dump(Data_Set2, to_write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retrieving Pickle\n",
    "with open('cleanscrape2.pickle','rb') as read_file:\n",
    "    new_df = pickle.load(read_file)\n",
    "new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting Dataset\n",
    "X = Data_Set2.drop('Score',1)\n",
    "y = Data_Set2['Score']\n",
    "#Hold 20% of the data out for testing\n",
    "X, X_test, y, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Training Data into Validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up the 3 models we're choosing from. Conduct multiple times with different splits to choose based on validation\n",
    "\n",
    "lm = LinearRegression()\n",
    "\n",
    "#Feature scaling for train, val, and test so that we can run our ridge model on each\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train.values)\n",
    "X_val_scaled = scaler.transform(X_val.values)\n",
    "X_test_scaled = scaler.transform(X_test.values)\n",
    "\n",
    "lm_reg = Ridge(alpha=1000)\n",
    "\n",
    "#Feature transforms for train, val, and test so that we can run our poly model on each\n",
    "poly = PolynomialFeatures(degree=2) \n",
    "\n",
    "X_train_poly = poly.fit_transform(X_train.values)\n",
    "X_val_poly = poly.transform(X_val.values)\n",
    "X_test_poly = poly.transform(X_test.values)\n",
    "\n",
    "lm_poly = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#validation\n",
    "\n",
    "lm.fit(X_train, y_train)\n",
    "print(f'Linear Regression val R^2: {lm.score(X_val, y_val):.3f}')\n",
    "\n",
    "lm_reg.fit(X_train_scaled, y_train)\n",
    "print(f'Ridge Regression val R^2: {lm_reg.score(X_val_scaled, y_val):.3f}')\n",
    "\n",
    "lm_poly.fit(X_train_poly, y_train)\n",
    "print(f'Degree 2 Polynomial Regression val R^2: {lm_poly.score(X_val_poly, y_val):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at coefficients for polynomial features\n",
    "lm_poly.fit(X_train_poly, y_train).coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conduct resulting test score\n",
    "lm.fit(X,y)\n",
    "print(f'Degree 2 Polynomial Regression test R^2: {lm_poly.score(X_test_poly, y_test):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizations of Residual Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare plots between validation and test\n",
    "plt.suptitle('Val Split Residuals',fontsize = 20, weight = 'bold',color='black', y=1.05)\n",
    "plt.title('Decent Prediction', fontsize=15)\n",
    "plt.xlabel('Score',fontsize = 20, weight = 'bold',color='black')\n",
    "plt.ylabel('Residuals',fontsize = 20, weight = 'bold',color='black')\n",
    "plt.xticks(fontsize = 10, weight = 'bold',color='blue')\n",
    "plt.yticks(fontsize = 10, weight = 'bold',color='blue')\n",
    "y_val_pred = lm.predict(X_val)\n",
    "sns.residplot(y_val_pred,y_val)\n",
    "plt.savefig(\"Val_Residuals.png\",dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.suptitle('Test Split Residuals',fontsize = 20, weight = 'bold',color='black', y=1.05)\n",
    "plt.title('Similar Prediction', fontsize=15)\n",
    "plt.xlabel('Score',fontsize = 20, weight = 'bold',color='black')\n",
    "plt.ylabel('Residuals',fontsize = 20, weight = 'bold',color='black')\n",
    "plt.xticks(fontsize = 10, weight = 'bold',color='blue')\n",
    "plt.yticks(fontsize = 10, weight = 'bold',color='blue')\n",
    "y_test_pred = lm.predict(X_test)\n",
    "sns.residplot(y_test_pred,y_test)\n",
    "plt.savefig(\"Test_Residuals.png\",dpi=300, bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}