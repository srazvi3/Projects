{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "import pickle\n",
    "import unidecode\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import MWETokenizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from nltk.chunk import ne_chunk\n",
    "from textblob import TextBlob\n",
    "from itertools import combinations\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from wordcloud import WordCloud, STOPWORDS \n",
    "import matplotlib.pyplot as plt \n",
    "import re\n",
    "import string\n",
    "from word2number import w2n\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "pyLDAvis.enable_notebook()\n",
    "import nltk"
   ]
  },
  {
   "source": [
    "# Scraping Game Scripts from Ace Attorney Wiki"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Function to obtain script data from Ace Attorney Wiki, also cleans up text slightly\n",
    "def get_script(url):\n",
    "    script = []\n",
    "    case_soup = requests.get(url)\n",
    "    case_trscpt = BeautifulSoup(case_soup.text)\n",
    "    case_txt = case_trscpt.find('div', class_='mw-parser-output')\n",
    "    for line in case_txt.find_all('p'):\n",
    "        script.append(line.text.replace('\\n',' ').strip(' '))\n",
    "    return script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get scripts for all the cases across the first five games, input is a list of urls for the transcripts of each game\n",
    "def get_cases(games):\n",
    "    games_corpus = []\n",
    "    for game in games:\n",
    "        games_corpus.append(get_script(game))\n",
    "        time.sleep(.5+2*random.random())\n",
    "    return games_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Links to the transcript pages on Ace Attorney Wiki, spaced out by game\n",
    "case_links = ['https://aceattorney.fandom.com/wiki/The_First_Turnabout_-_Transcript',\n",
    "'https://aceattorney.fandom.com/wiki/Turnabout_Sisters_-_Transcript_-_Part_1',\n",
    "'https://aceattorney.fandom.com/wiki/Turnabout_Sisters_-_Transcript_-_Part_2',\n",
    "'https://aceattorney.fandom.com/wiki/Turnabout_Samurai_-_Transcript_-_Part_1',\n",
    "'https://aceattorney.fandom.com/wiki/Turnabout_Samurai_-_Transcript_-_Part_2',\n",
    "'https://aceattorney.fandom.com/wiki/Turnabout_Samurai_-_Transcript_-_Part_3',\n",
    "'https://aceattorney.fandom.com/wiki/Turnabout_Goodbyes_-_Transcript_-_Part_1',\n",
    "'https://aceattorney.fandom.com/wiki/Turnabout_Goodbyes_-_Transcript_-_Part_2',\n",
    "'https://aceattorney.fandom.com/wiki/Turnabout_Goodbyes_-_Transcript_-_Part_3',\n",
    "'https://aceattorney.fandom.com/wiki/Rise_from_the_Ashes_-_Transcript_-_Part_1',\n",
    "'https://aceattorney.fandom.com/wiki/Rise_from_the_Ashes_-_Transcript_-_Part_2',\n",
    "'https://aceattorney.fandom.com/wiki/Rise_from_the_Ashes_-_Transcript_-_Part_3',\n",
    "\n",
    "'https://aceattorney.fandom.com/wiki/The_Lost_Turnabout_-_Transcript',\n",
    "'https://aceattorney.fandom.com/wiki/Reunion,_and_Turnabout_-_Transcript_-_Part_1',\n",
    "'https://aceattorney.fandom.com/wiki/Reunion,_and_Turnabout_-_Transcript_-_Part_2',\n",
    "'https://aceattorney.fandom.com/wiki/Turnabout_Big_Top_-_Transcript_-_Part_1',\n",
    "'https://aceattorney.fandom.com/wiki/Turnabout_Big_Top_-_Transcript_-_Part_2',\n",
    "'https://aceattorney.fandom.com/wiki/Farewell,_My_Turnabout_-_Transcript_-_Part_1',\n",
    "'https://aceattorney.fandom.com/wiki/Farewell,_My_Turnabout_-_Transcript_-_Part_2',\n",
    "\n",
    "'https://aceattorney.fandom.com/wiki/Turnabout_Memories_-_Transcript',\n",
    "'https://aceattorney.fandom.com/wiki/The_Stolen_Turnabout_-_Transcript_-_Part_1',\n",
    "'https://aceattorney.fandom.com/wiki/The_Stolen_Turnabout_-_Transcript_-_Part_2',\n",
    "'https://aceattorney.fandom.com/wiki/Recipe_for_Turnabout_-_Transcript_-_Part_1',\n",
    "'https://aceattorney.fandom.com/wiki/Recipe_for_Turnabout_-_Transcript_-_Part_2',\n",
    "'https://aceattorney.fandom.com/wiki/Turnabout_Beginnings_-_Transcript',\n",
    "'https://aceattorney.fandom.com/wiki/Bridge_to_the_Turnabout_-_Transcript_-_Part_1',\n",
    "'https://aceattorney.fandom.com/wiki/Bridge_to_the_Turnabout_-_Transcript_-_Part_2',\n",
    "\n",
    "'https://aceattorney.fandom.com/wiki/Turnabout_Trump_-_Transcript',\n",
    "'https://aceattorney.fandom.com/wiki/Turnabout_Corner_-_Transcript_-_Part_1',\n",
    "'https://aceattorney.fandom.com/wiki/Turnabout_Corner_-_Transcript_-_Part_2',\n",
    "'https://aceattorney.fandom.com/wiki/Turnabout_Serenade_-_Transcript_-_Part_1',\n",
    "'https://aceattorney.fandom.com/wiki/Turnabout_Serenade_-_Transcript_-_Part_2',\n",
    "'https://aceattorney.fandom.com/wiki/Turnabout_Succession_-_Transcript_-_Part_1',\n",
    "'https://aceattorney.fandom.com/wiki/Turnabout_Succession_-_Transcript_-_Part_2',\n",
    "\n",
    "'https://aceattorney.fandom.com/wiki/Turnabout_Countdown_-_Transcript',\n",
    "'https://aceattorney.fandom.com/wiki/The_Monstrous_Turnabout_-_Transcript_-_Part_1',\n",
    "'https://aceattorney.fandom.com/wiki/The_Monstrous_Turnabout_-_Transcript_-_Part_2',\n",
    "'https://aceattorney.fandom.com/wiki/Turnabout_Academy_-_Transcript_-_Part_1',\n",
    "'https://aceattorney.fandom.com/wiki/Turnabout_Academy_-_Transcript_-_Part_2',\n",
    "'https://aceattorney.fandom.com/wiki/The_Cosmic_Turnabout_-_Transcript',\n",
    "'https://aceattorney.fandom.com/wiki/Turnabout_for_Tomorrow_-_Transcript',\n",
    "'https://aceattorney.fandom.com/wiki/Turnabout_Reclaimed_-_Transcript']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtain corpus separated by case transcript links and pickle it\n",
    "with open('corpusscrape.pickle', 'wb') as to_write:\n",
    "    pickle.dump(get_cases(case_links), to_write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retrieving Pickle\n",
    "with open('corpusscrape.pickle','rb') as read_file:\n",
    "    corpus = pickle.load(read_file)"
   ]
  },
  {
   "source": [
    "# Text Preprocessing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Corpus is broken up by transcript page, so this function puts the elements into one list\n",
    "def combine_parts(list,x,y):\n",
    "    new_list = []\n",
    "    for i in range(x,y):\n",
    "        new_list = new_list + list[i]\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get game specific corpuses in order to track character interactions across games as well as a corpus of all the games\n",
    "aa1 = combine_parts(corpus,0,12)\n",
    "aa2 = combine_parts(corpus,12,19)\n",
    "aa3 = combine_parts(corpus,19,27)\n",
    "aa4 = combine_parts(corpus,27,34)\n",
    "aa5 = combine_parts(corpus,34,42)\n",
    "aa1_5 = combine_parts(corpus,0,42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Based on different dialogue choices, the transcript indicates what the next line is with leads to or leads back to, so I'm removing them because it's redundant\n",
    "def remove_leads(corp):\n",
    "    new_list = []\n",
    "    for line in corp:\n",
    "        if line[:5] != 'Leads':\n",
    "            new_list.append(line)\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing redundant game dialogue boxes\n",
    "aa1_5 = remove_leads(aa1_5)\n",
    "aa1 = remove_leads(aa1)\n",
    "aa2 = remove_leads(aa2)\n",
    "aa3 = remove_leads(aa3)\n",
    "aa4 = remove_leads(aa4)\n",
    "aa5 = remove_leads(aa5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to change pos_tag to format for wordnetlemmatizer\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess text\n",
    "def txt_preprocess(case):\n",
    "    new_list = []\n",
    "    for doc in case:\n",
    "        #remove punctuation and white space\n",
    "        doc = re.sub('[%s]' % re.escape(string.punctuation), '', doc)\n",
    "        #remove accented characters\n",
    "        doc = unidecode.unidecode(doc)\n",
    "        #lowercase letters\n",
    "        doc = doc.lower()\n",
    "        #remove numbers\n",
    "        doc = re.sub('\\w*\\d\\w*', ' ', doc)\n",
    "        new_list.append(doc)\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Game by game script preprocessing\n",
    "aa1 = txt_preprocess(aa1)\n",
    "aa2 = txt_preprocess(aa2)\n",
    "aa3 = txt_preprocess(aa3)\n",
    "aa4 = txt_preprocess(aa4)\n",
    "aa5 = txt_preprocess(aa5)\n",
    "aa1_5 = txt_preprocess(aa1_5)"
   ]
  },
  {
   "source": [
    "# LDA on Full Corpus"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count Vectorizing the entire corpus aa1_5\n",
    "cv = CountVectorizer(stop_words='english',token_pattern = r'\\b[a-zA-Z]{3,}\\b',analyzer='word',max_df = 0.5, min_df = 10, max_features=3000)\n",
    "dtm_tf = cv.fit_transform(aa1_5)\n",
    "Char = pd.DataFrame(dtm_tf.toarray(), columns=cv.get_feature_names())\n",
    "Char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TFIDF Doc-Term Matrix\n",
    "tfidf_vectorizer = TfidfVectorizer(**cv.get_params())\n",
    "dtm_tfidf = tfidf_vectorizer.fit_transform(Char)\n",
    "dtm_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for TF DTM\n",
    "lda_tf = LatentDirichletAllocation(n_components=10, random_state=0)\n",
    "lda_tf.fit(dtm_tf)\n",
    "\n",
    "# for TFIDF DTM\n",
    "lda_tfidf = LatentDirichletAllocation(n_components=10, random_state=0)\n",
    "lda_tfidf.fit(dtm_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create webpage to record analyzing topics\n",
    "full = pyLDAvis.sklearn.prepare(lda_tfidf, dtm_tfidf, cv)\n",
    "pyLDAvis.save_html(full, 'full.html')"
   ]
  },
  {
   "source": [
    "# LSA on Character Interactions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finds list of indexes that contain two character lines and when they are talking to each other or in the same group discussion (distance of <=2 indexes)\n",
    "def interlist(char1,char2):    \n",
    "    interaction = []\n",
    "    for ind in char1:\n",
    "        if ind - 2 in char2:\n",
    "            interaction.append(ind-2)\n",
    "            interaction.append(ind)\n",
    "        if ind - 1 in char2:\n",
    "            interaction.append(ind-1)\n",
    "            interaction.append(ind)\n",
    "        if ind + 1 in char2:\n",
    "            interaction.append(ind+1)\n",
    "            interaction.append(ind)\n",
    "        if ind + 2 in char2:\n",
    "            interaction.append(ind+2)\n",
    "            interaction.append(ind)\n",
    "    inter = sorted(list(set(interaction)))\n",
    "    return inter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List of indexes for each specific character\n",
    "nick = list(Char[(Char['phoenix'] > 0)].index)\n",
    "maya = list(Char[(Char['maya'] > 0)].index)\n",
    "apollo = list(Char[(Char['apollo'] > 0)].index)\n",
    "miles = list(Char[(Char['edgeworth'] > 0)].index)\n",
    "fran = list(Char[(Char['von'] > 0) &(Char['karma'] > 0)].index)\n",
    "godot = list(Char[(Char['godot'] > 0)].index)\n",
    "klav = list(Char[(Char['klavier'] > 0)].index)\n",
    "black = list(Char[(Char['blackquill'] > 0)].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List of indexes for interaction analysis\n",
    "int1 = [aa1_5[i] for i in interlist(miles,nick)]\n",
    "int2 = [aa1_5[i] for i in interlist(fran,nick)]\n",
    "int3 = [aa1_5[i] for i in interlist(godot,nick)]\n",
    "int4 = [aa1_5[i] for i in interlist(klav,apollo)]\n",
    "int5 = [aa1_5[i] for i in interlist(black,nick)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Interaction related Doc-Term Matrix\n",
    "vectorizer = CountVectorizer(stop_words='english',max_df = 0.5, min_df = 10, max_features=3000)\n",
    "doc_word = vectorizer.fit_transform(int5)\n",
    "doc_word.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_lsa = pd.DataFrame(doc_word.toarray(), index=int5, columns=vectorizer.get_feature_names()).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSA modeling across chosen number of topics, here 5\n",
    "lsa = TruncatedSVD(5)\n",
    "doc_topic = lsa.fit_transform(doc_word)\n",
    "lsa.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pull words from Matrix\n",
    "topic_word = pd.DataFrame(lsa.components_.round(3),\n",
    "             index = [\"component_1\",\"component_2\",\"component_3\",\"component_4\",\"component_5\"],\n",
    "             columns = vectorizer.get_feature_names())\n",
    "topic_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Produce top words from each topic\n",
    "def display_topics(model, feature_names, no_top_words, topic_names=None):\n",
    "    for ix, topic in enumerate(model.components_):\n",
    "        if not topic_names or not topic_names[ix]:\n",
    "            print(\"\\nTopic \", ix)\n",
    "        else:\n",
    "            print(\"\\nTopic: '\",topic_names[ix],\"'\")\n",
    "        print(\", \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_topics(lsa, vectorizer.get_feature_names(), 5)"
   ]
  },
  {
   "source": [
    "# Word Clouds for each Doc-Term Matrix"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Cloud Generator\n",
    "df = pd.DataFrame(Char) \n",
    "  \n",
    "comment_words = '' \n",
    "stopwords = set(STOPWORDS) \n",
    "  \n",
    "\n",
    "for val in df[0]: \n",
    "      \n",
    "    \n",
    "    val = str(val) \n",
    "  \n",
    "    \n",
    "    tokens = val.split() \n",
    "      \n",
    "    \n",
    "    for i in range(len(tokens)): \n",
    "        tokens[i] = tokens[i].lower() \n",
    "      \n",
    "    comment_words += \" \".join(tokens)+\" \"\n",
    "  \n",
    "wordcloud = WordCloud(width = 800, height = 800, \n",
    "                background_color ='white', \n",
    "                stopwords = stopwords, \n",
    "                min_font_size = 10).generate(comment_words) \n",
    "  \n",
    "\n",
    "plt.figure(figsize = (8, 8), facecolor = None) \n",
    "plt.imshow(wordcloud) \n",
    "plt.axis(\"off\") \n",
    "plt.tight_layout(pad = 0) \n",
    "  \n",
    "plt.show() "
   ]
  }
 ]
}